{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import credentials  # Import api/access_token keys from credentials.py\n",
    "import settings  # Import related setting constants from settings.py\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tweepy\n",
    "import pickle\n",
    "import mysql.connector\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-8.0.21-cp37-cp37m-manylinux1_x86_64.whl (15.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.8 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.0.0 in /home/shubham/anaconda3/lib/python3.7/site-packages (from mysql-connector-python) (3.12.2)\n",
      "Requirement already satisfied: six>=1.9 in /home/shubham/anaconda3/lib/python3.7/site-packages (from protobuf>=3.0.0->mysql-connector-python) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /home/shubham/anaconda3/lib/python3.7/site-packages (from protobuf>=3.0.0->mysql-connector-python) (45.2.0.post20200210)\n",
      "Installing collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-8.0.21\n"
     ]
    }
   ],
   "source": [
    "#!pip install tweepy\n",
    "#!pip install textblob\n",
    "#!pip install keras\n",
    "#!pip install pickle\n",
    "!pip install mysql-connector-python\n",
    "#!pip install joblib\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')\n",
    "tokenizer = joblib.load('tokenizer.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStreamListener(tweepy.StreamListener):\n",
    "\n",
    "    def on_status(self, status):\n",
    "        '''\n",
    "        Extract info from tweets\n",
    "        '''\n",
    "        try:\n",
    "            if status.retweeted:\n",
    "                # Avoid retweeted info, and only original tweets will be received\n",
    "                return True\n",
    "            # Extract attributes from each tweet\n",
    "            id_str = status.id_str\n",
    "            created_at = status.created_at\n",
    "            text = self.lemitizor(status.text)  # Pre-processing the text\n",
    "            sentiment = predict(text)\n",
    "            user_created_at = status.user.created_at\n",
    "            user_location = self.deEmojify(status.user.location)\n",
    "            user_description = self.deEmojify(status.user.description)\n",
    "            user_followers_count = status.user.followers_count\n",
    "            longitude = None\n",
    "            latitude = None\n",
    "            if status.coordinates:\n",
    "                longitude = status.coordinates['coordinates'][0]\n",
    "                latitude = status.coordinates['coordinates'][1]\n",
    "\n",
    "            retweet_count = status.retweet_count\n",
    "            favorite_count = status.favorite_count\n",
    "\n",
    "            # Store all data in MySQL\n",
    "            if mydb.is_connected():\n",
    "                mycursor = mydb.cursor()\n",
    "                sql = \"INSERT INTO {} (id_str, created_at, text, sentiment, user_created_at, user_location, user_description, user_followers_count, longitude, latitude, retweet_count, favorite_count) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\".format(\n",
    "                    settings.TABLE_NAME)\n",
    "                val = (id_str, created_at, text, sentiment, user_created_at, user_location,user_description, user_followers_count, longitude, latitude, retweet_count, favorite_count)\n",
    "                mycursor.execute(sql, val)\n",
    "                mydb.commit()\n",
    "                mycursor.close()\n",
    "                print(text)\n",
    "                print(\"Long: {}, Lati: {}\".format(longitude, latitude))\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        '''\n",
    "        Since Twitter API has rate limits, stop srcraping data as it exceed to the thresold.\n",
    "        '''\n",
    "        if status_code == 420:\n",
    "            # return False to disconnect the stream\n",
    "            return False\n",
    "\n",
    "    def clean_tweet(self, tweet):\n",
    "        '''\n",
    "        Use sumple regex statemnents to clean tweet text by removing links and special characters\n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "    def deEmojify(self,text):\n",
    "        '''\n",
    "        Strip all non-ASCII characters to remove emoji characters\n",
    "        '''\n",
    "        if text:\n",
    "            return text.encode('ascii', 'ignore').decode('ascii')\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def lemitizor(self, text):\n",
    "        try:\n",
    "            lemet = WordNetLemmatizer()\n",
    "            if text:\n",
    "                text = self.clean_tweet(text)\n",
    "                text = text.replace('RT','')\n",
    "                text = self.deEmojify(text)\n",
    "                tokenized_tweet = text.split()\n",
    "                lemetized_tweet = []\n",
    "                for i in tokenized_tweet:\n",
    "                    lemetized_tweet.append(lemet.lemmatize(i)) # lemmitizing\n",
    "                return ' '.join(lemetized_tweet)\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sentiment(score, include_neutral=True):\n",
    "        SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
    "        if include_neutral:\n",
    "            label = 0\n",
    "            if score <= SENTIMENT_THRESHOLDS[0]:\n",
    "                label = -1\n",
    "            elif score >= SENTIMENT_THRESHOLDS[1]:\n",
    "                label = 1\n",
    "\n",
    "            return label\n",
    "        else:\n",
    "            return -1 if score < 0.5 else 1\n",
    "\n",
    "def predict(text, include_neutral=True):\n",
    "        SEQUENCE_LENGTH = 300\n",
    "        x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
    "        # Predict\n",
    "        score = model.predict([x_test])[0]\n",
    "        # Decode sentiment\n",
    "        label = decode_sentiment(score, include_neutral=include_neutral)\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    database=\"twitterdb\",\n",
    "    charset = 'utf8'\n",
    ")\n",
    "if mydb.is_connected():\n",
    "    '''\n",
    "    Check if this table exits. If not, then create a new one.\n",
    "    '''\n",
    "    mycursor = mydb.cursor()\n",
    "    mycursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_name = '{0}'\n",
    "        \"\"\".format(settings.TABLE_NAME))\n",
    "    if mycursor.fetchone()[0] != 1:\n",
    "        mycursor.execute(\"CREATE TABLE {} ({})\".format(settings.TABLE_NAME, settings.TABLE_ATTRIBUTES))\n",
    "        mydb.commit()\n",
    "    mycursor.close()\n",
    "\n",
    "auth = tweepy.OAuthHandler(credentials.API_KEY, credentials.API_SECRET_KEY)\n",
    "auth.set_access_token(credentials.ACCESS_TOKEN, credentials.ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "myStreamListener = MyStreamListener()\n",
    "myStream = tweepy.Stream(auth = api.auth, listener = myStreamListener)\n",
    "myStream.filter(languages=[\"en\"], track = settings.TRACK_WORDS,locations=[68.1766451354, 7.96553477623, 97.4025614766, 35.4940095078])\n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
